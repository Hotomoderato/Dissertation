{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "i3e_data=pd.read_csv('2016.csv')\n",
    "print(i3e_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_Date=i3e_data['Date'].dtypes\n",
    "type_Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "dt = i3e_data[\"Date\"]\n",
    "\n",
    "# dt = datetime.strptime(dt, '%Y-%m-%d %H:%M:%S')\n",
    "dt = dt.apply(lambda x:datetime.strptime(x, '%Y/%m/%d'))\n",
    "\n",
    "i3e_data[\"year_now\"] = dt.map(lambda x: x.year)\n",
    "i3e_data['age of building'] = i3e_data['year_now'] - i3e_data['year of construction']\n",
    "i3e_data.drop(columns=['year_now', 'Date', 'YYYYMM', 'USPD', 'type'], inplace=True)\n",
    "i3e_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "|serial|name|description|type|\n",
    "|:--:|:--:|:--|:--:|\n",
    "| (A)| Date |date in Windows format.|Date\n",
    "| (B)| M1, t |mass of the input water (heat carrier) per day.|Ratio\n",
    "| (C)| М2, t |mass of the output water. If the residential building has the open heating system (hot water is flowed from the heating system), (C) less than (B).|Ratio\n",
    "| (D)| ΔМ, t |difference in volumes (C)-(B). For buildings with the opened heating system this is the data for analysis. In closed system it is the technological parameter allows observation for equipment.|Ratio\n",
    "| (E)| Т1, °C |average temperature of the heating carrier in the input of the heating system. It is the independent variable from home characteristics.|Interval\n",
    "| (F)| Т2, °C |average temperature of the heating carrier in the output. It is the dependent variable both from (E) and heating consumption at building.|Interval\n",
    "| (G)| ΔТ, °C |temperature difference, (F)-(E).|Interval\n",
    "| (H)| Q, Gcal |amount of the consumed heating in Gcal. It is calculated by formula.|Ratio\n",
    "| (I)| USPD |ID of the heating meter. Some residential buildings have many heating meters.|Nominal\n",
    "| (J)| YYYYMM |date in the format year-month YYYYMM.|Date\n",
    "| (K)| registrated |what is registrated, heating or heating plus hot water.|Nominal\n",
    "| (L)| scheme |type of the heating system (opened or closed).|Nominal\n",
    "| (M)| type |code system-load (4 digits). First digit 1 is opened system, 2 is closed system. The second digit 0 is heating, 1 is heating and hot water supply. The third and fourth digits is floors amount (01, 02, 03, ..., 17).|Nominal\n",
    "| (N)| area |area of building that heating meter is served.|Ratio\n",
    "| (O)| floors |amount of building floors. |Ordinal\n",
    "| (P)| walls material |walls material.|Nominal\n",
    "| (Q)| year of construction |year of building construction.|Date\n",
    "| (R)| area of building |total area of building.|Ratio\n",
    "| (S)| temp,˚C |outdoor temperature.|Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = LabelEncoder().fit_transform(i3e_data['scheme'])\n",
    "df_scheme = pd.DataFrame(c)\n",
    "df_scheme.columns=['scheme']\n",
    "\n",
    "c = LabelEncoder().fit_transform(i3e_data['floors'])\n",
    "df_floors = pd.DataFrame(c)\n",
    "df_floors.columns=['floors']\n",
    "\n",
    "c = LabelEncoder().fit_transform(i3e_data['walls material'])\n",
    "df_wallsMaterial = pd.DataFrame(c)\n",
    "df_wallsMaterial.columns=['walls materials']\n",
    "\n",
    "c = LabelEncoder().fit_transform(i3e_data['registrated'])\n",
    "df_registered = pd.DataFrame(c)\n",
    "df_registered.columns=['registrated']\n",
    "\n",
    "df_x=np.concatenate([df_scheme,df_wallsMaterial,df_registered,df_floors],axis=-1)\n",
    "\n",
    "df_Label =pd.DataFrame(df_x)\n",
    "data_Label = i3e_data.drop(columns=['scheme', 'registrated', 'floors', 'walls material'])\n",
    "data_Label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in df_Label.columns:\n",
    "    data_Label[str(item)]=df_Label[item].values\n",
    "\n",
    "data_Label.rename(columns={'0':'scheme', '1':'walls material', '2':'registered', '3':'floors'}, inplace=True)\n",
    "print (data_Label.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# 取出这三类数据\n",
    "data_k = data_Label[['ΔМ, t','ΔТ, °C','Q, Gcal']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K值查找\n",
    "distor =[]\n",
    "for i in range(1,15):\n",
    "    km = KMeans(n_clusters =i)\n",
    "    km.fit(data_k)\n",
    "    distor.append(km.inertia_)\n",
    "plt.plot(range(1,15),distor,marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans聚类训练\n",
    "mod = KMeans(n_clusters=7,random_state=255)\n",
    "y_pre = mod.fit_predict(data_k)\n",
    "# 聚类的数目\n",
    "r1 = pd.Series(mod.labels_).value_counts()\n",
    "# 聚类的3个质心\n",
    "r2 = pd.DataFrame(mod.cluster_centers_)\n",
    "# 将获取到的聚类的数目以及获取的质心合并\n",
    "r = pd.concat([r1,r2],axis = 1)\n",
    "r.columns = [\"count\",\"ΔМ, t\",\"ΔТ, °C\",\"Q, Gcal\"]\n",
    "data_Label[\"kmeans_class\"] = y_pre\n",
    "display(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除异常值\n",
    "data_k = data_k[~(data_k['ΔМ, t'].isin([5173.2]) | data_k['ΔТ, °C'].isin([25.22]))]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K值查找\n",
    "distor =[]\n",
    "for i in range(1,15):\n",
    "    km = KMeans(n_clusters =i)\n",
    "    km.fit(data_k)\n",
    "    distor.append(km.inertia_)\n",
    "plt.plot(range(1,15),distor,marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans聚类训练\n",
    "mod = KMeans(n_clusters=6,random_state=255)\n",
    "y_pre = mod.fit_predict(data_k)\n",
    "# 聚类的数目\n",
    "r1 = pd.Series(mod.labels_).value_counts()\n",
    "# 聚类的质心\n",
    "r2 = pd.DataFrame(mod.cluster_centers_)\n",
    "# 将获取到的聚类的数目以及获取的质心合并\n",
    "r = pd.concat([r1,r2],axis = 1)\n",
    "r.columns = [\"count\",\"ΔМ, t\",\"ΔТ, °C\",\"Q, Gcal\"]\n",
    "data_k[\"kmeans_class\"] = y_pre\n",
    "display(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_Label.drop(columns=['Q, Gcal'])\n",
    "y = data_Label['Q, Gcal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = X\n",
    "Y_pca = y\n",
    "X_pca_train, X_pca_test, y_pca_train, y_pca_test = train_test_split(X_pca, Y_pca, test_size=0.3)\n",
    "\n",
    "# Create scaler: scaler\n",
    "scaler = StandardScaler()\n",
    "# Create a PCA instance: pca\n",
    "pca = PCA()\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, pca)\n",
    "# Fit the pipeline to 'samples'\n",
    "pipeline.fit(X_pca)\n",
    "# Plot the explained variances\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_)\n",
    "plt.xlabel('PCA feature')\n",
    "plt.ylabel('variance')\n",
    "plt.xticks(features)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=6)\n",
    "X_pca_new= pd.DataFrame(pca.fit_transform(X_pca))\n",
    "X_pca_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sweetviz as sv\n",
    "my_report = sv.analyze(data_Label)\n",
    "my_report.show_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(data_Label['walls material']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from imblearn.over_sampling import RandomOverSampler  # 随机重复采样\n",
    "from imblearn.over_sampling import SMOTE  # 选取少数类样本插值采样\n",
    "X_smo = data_Label.drop(columns=['walls material'])\n",
    "y_smo = data_Label['walls material']\n",
    "smo = SMOTE(random_state=42)\n",
    "X_smo, y_smo = smo.fit_resample(X_smo, y_smo.astype('int'))\n",
    "\n",
    "print(Counter(y_smo))\n",
    "y_smo\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([X_smo, y_smo],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sweetviz as sv\n",
    "my_report = sv.analyze(data)\n",
    "my_report.show_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_Label.drop(columns=['М2, t', 'Q, Gcal', 'ΔМ, t', 'Т2, °C', 'ΔТ, °C', 'year of construction'])\n",
    "y = data_Label['Q, Gcal']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_pca_new, Y_pca, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "gbm_param_grid = {\n",
    "    'colsample_bytree': [0.5],\n",
    "    'n_estimators': [500],\n",
    "    'max_depth': [5]\n",
    "}\n",
    "\n",
    "gbm = xgb.XGBRegressor(objective ='reg:squarederror')\n",
    "\n",
    "grid_mse = GridSearchCV(estimator=gbm,param_grid=gbm_param_grid,scoring='neg_mean_squared_error',cv=2,verbose=1)\n",
    "\n",
    "# Fit grid_mse to the data\n",
    "grid_mse.fit(X_pca_new, Y_pca)\n",
    "grid_preds=grid_mse.predict(X_test)\n",
    "\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))\n",
    "r2 = r2_score(y_test, grid_preds)\n",
    "print('R2: {:.3f}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "grid_mse.fit(X_train, y_train, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=True)\n",
    "# make predictions for test data\n",
    "y_pred = grid_mse.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = r2_score(y_test, grid_preds)\n",
    "print(\"Accuracy: %.2f%%\" % accuracy)\n",
    "# retrieve performance metrics\n",
    "results = grid_mse.evals_result()\n",
    "epochs = len(results['validation_0']['error'])\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "# plot log loss\n",
    "fig1, ax = pyplot.subplots()\n",
    "ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n",
    "ax.legend()\n",
    "pyplot.ylabel('Log Loss')\n",
    "pyplot.title('XGBoost Log Loss')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from minisom import MiniSom\n",
    "import math\n",
    "X_som = data_Label[['ΔМ, t','ΔТ, °C','Q, Gcal']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_som, y, test_size=0.3, random_state=42)\n",
    "N = X_train.shape[0]  #样本数量\n",
    "M = X_train.shape[1]  #维度/特征数量\n",
    "#设置超参数\n",
    "size = math.ceil(np.sqrt(5 * np.sqrt(N)))  # 经验公式：决定输出层尺寸\n",
    "print(\"训练样本个数:{}  测试样本个数:{}\".format(N,X_test.shape[0]))\n",
    "print(\"输出网格最佳边长为:\",size)\n",
    "max_iter = 200\n",
    "# Initialization and training\n",
    "som = MiniSom(size, size, M, sigma=3, learning_rate=0.5, neighborhood_function='bubble')\n",
    "som.random_weights_init(X_train)\n",
    "som.train_batch(X_train, max_iter, verbose=False)\n",
    "winmap = som.labels_map(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers import Dense,Dropout,LSTM,Bidirectional,Embedding\n",
    "#from keras.layers import Embedding\n",
    "from sklearn.metrics import r2_score\n",
    "from keras.models import  Sequential\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "#from keras.datasets import imdb\n",
    "\n",
    "look_back=3   #步长（你需要跳的就是这个参数）\n",
    "percent=0.3    #测试集与训练集的比例\n",
    "#max_features=trainx.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#归一化\n",
    "scaler=MinMaxScaler()\n",
    "scaler_datax=scaler.fit_transform(X_train)\n",
    "scaler_datay=scaler.fit_transform(y_train.values.reshape(-1,1))\n",
    "\n",
    "scaler_trainx, scaler_testx, scaler_trainy, scaler_testy = train_test_split(scaler_datax, scaler_datay, test_size=percent, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化\n",
    "trainx=[]\n",
    "trainy=[]\n",
    "testx=[]\n",
    "testy=[]\n",
    "\n",
    "for i in range(scaler_trainx.shape[0] - look_back):\n",
    "    a = scaler_trainx[i:(i + look_back),:]    \n",
    "    a = a.transpose()  \n",
    "    trainx.append(a)\n",
    "    trainy.append(scaler_trainy[i + look_back, 0])   \n",
    "    \n",
    "for i in range(scaler_testx.shape[0] - look_back):\n",
    "    a =scaler_testx[i:(i + look_back),:]    \n",
    "    a = a.transpose()  \n",
    "    testx.append(a)\n",
    "    testy.append(scaler_testy[i + look_back, 0])   \n",
    "\n",
    "#将数据变成array格式作为最终的输入\n",
    "trainx=np.array(trainx)\n",
    "trainy=np.array(trainy)\n",
    "testx=np.array(testx)\n",
    "testy=np.array(testy)\n",
    "print(trainx.shape,trainy.shape,testx.shape,testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#搭建模型\n",
    "model=Sequential()\n",
    "#model.add(Embedding(max_features,128,input_length=trainx.shape[0]))\n",
    "#神经元个数50，可调\n",
    "model.add(Bidirectional(LSTM(50,input_shape=(trainx.shape[1],trainx.shape[2]))))\n",
    "model.add(Dropout(0.3))   #辍学率，可调\n",
    "model.add(Dense(1,activation='sigmoid'))   #sigmoid\n",
    "model.compile(optimizer='adam',loss='mean_squared_error')\n",
    "#可调迭代次数，batch_size\n",
    "model.fit(trainx,trainy,batch_size=32,epochs = 10,verbose=2)\n",
    "predict=model.predict(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mape指标（可能用得上）\n",
    "def mape(test_y,predict_y):\n",
    "    return np.mean(np.abs(((predict_y-test_y)/test_y)))*100\n",
    "\n",
    "#反归一化\n",
    "predict=scaler.inverse_transform(predict)\n",
    "testy=scaler.inverse_transform(testy.reshape(-1,1))\n",
    "\n",
    "#可视化\n",
    "plt.figure(figsize=(13,8))  #大小，颜色都可调\n",
    "plt.plot(predict,c='red',label='predict')\n",
    "plt.plot(testy,c='brown',label='true')\n",
    "plt.legend();\n",
    "\n",
    "#指标衡量（MAE,RMSE,MAPE）\n",
    "print('r2: %.5f' %r2_score(predict,testy))\n",
    "mae = mean_absolute_error(predict,testy)\n",
    "rmse = np.sqrt(mean_squared_error(predict,testy))      \n",
    "print(' MAE : %.5f ' %(mae))\n",
    "print(' RMSE : %.5f ' %(rmse))\n",
    "print(' MAPE : %.6f ' %((mape(predict,testy))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import *\n",
    "import os\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "import sklearn\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Bidirectional\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from gensim.models import word2vec, KeyedVectors\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#import jieba\n",
    "import re\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs):\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, trainx.shape[1]))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(trainx.shape[1], activation='softmax')(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])\n",
    "    return output_attention_mul\n",
    "\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=10, mode='auto')\n",
    "# model.fit(train_x, train_y, batch_size=32, epochs=5, validation_split=0.1, callbacks=[reduce_lr])\n",
    "callbacks = [EarlyStopping(monitor='val_loss', verbose=1, patience=50),\n",
    "\t\t\t ModelCheckpoint(\"model.hdf5\", monitor='val_loss',\n",
    "\t\t\t\t\t\t\t mode='min', verbose=0, save_best_only=True)]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义当误差为最小时保存最优模型，patience=1111表示1111个epoch损失都没有下降那么可以提前停止\n",
    "#lsym.hdf5为保存的模型名称，monitor为观测值也就是val_loss，mode='min'代表最小的val_loss\n",
    "# model1 = Sequential()\n",
    "inputs = Input(shape=(trainx.shape[1], trainx.shape[2]))\n",
    "context1 = GRU(3, return_sequences=True)(inputs)  #lstm神经元数量为48个 激活函数为relu\n",
    "# context1=tensorflow.keras.layers.Dropout(0.5)(context1)\n",
    "atten = attention_3d_block(context1)\n",
    "# con2=Conv1D(48, 3, padding='same')(atten )\n",
    "# atten=tensorflow.keras.layers.GlobalAveragePooling1D()(con2)\n",
    "atten = Flatten()(atten)\n",
    "x = Dense(128, activation='relu')(atten)# 全连接层,全连接层神经元维度为48\n",
    "x = Dense(128, activation='relu')(x)# 全连接层,全连接层神经元维度为48\n",
    "x = Dense(128, activation='relu')(x)# 全连接层,全连接层神经元维度为48\n",
    "# x = Dense(1024, activation='relu')(x)# 全连接层,全连接层神经元维度为48\n",
    "output = Dense(1, activation='relu')(x)  #softmax层\n",
    "model = Model(inputs=[inputs], outputs=output)\n",
    "model.compile(loss='mean_squared_error', optimizer='adam',metrics =[\"mae\"])   #损失为mse ，优化器为adam\n",
    "#model.summary()\n",
    "\n",
    "# fit network\n",
    "LSTM = model.fit(trainx, trainy, epochs=300, batch_size=32, callbacks=callbacks,\n",
    "                 validation_data=(X_test, y_test), verbose=1)\n",
    "#callbacks为上面定义的保存最优模型用的，validation_data为验证集，verbose代表每个epoch都打印\n",
    "\n",
    "model_new = load_model(\"model.hdf5\")  #加载最好的模型\n",
    "model_new.evaluate(testx, testy) \n",
    "model.evaluate(testx, testy)  #计算测试集的val_loss\n",
    "y_pre=list(np.squeeze(model.predict(testx)))\n",
    "\n",
    "y_pre = pd.Series(np.array(y_pre))\n",
    "y_test = pd.Series(np.array(testy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'y_pre':y_pre,'y_test':y_test})\n",
    "\n",
    "#评价指标\n",
    "def mape(test_y,predict_y):\n",
    "    return np.mean(np.abs(((predict_y-test_y)/test_y)))*100\n",
    "\n",
    "r2 = r2_score(y_pre,y_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_pre,y_test))\n",
    "mae = mean_absolute_error(y_pre,y_test)\n",
    "\n",
    "print(' R2 : %.5F'  %(r2))           \n",
    "print(' MAE : %.5f ' %(mae))\n",
    "print(' RMSE : %.5f ' %(rmse))\n",
    "print(' MAPE : %.5f ' %((mape(y_pre,y_test))))\n",
    "\n",
    "sns.set()\n",
    "fig,ax = plt.subplots(figsize=(13,8))\n",
    "ax.plot(y_test,c='r',label='True')\n",
    "ax.plot(y_pre,c='g',label='Predict')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('dissertation': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "metadata": {
   "interpreter": {
    "hash": "834ff2541ea382734cf9e983d04cdc412954ff861694b11af9c06f1cb05894f9"
   }
  },
  "interpreter": {
   "hash": "834ff2541ea382734cf9e983d04cdc412954ff861694b11af9c06f1cb05894f9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}